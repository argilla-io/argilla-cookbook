{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔍 Monitor and Evaluate a RAG application with Argilla\n",
    "\n",
    "In this tutorial, we will show you how to monitor and evaluate a RAG system with Argilla. We will use the [Argilla](https://argilla.ai/) span handler to log the answers to the questions asked to the RAG system. We will also use the [Wikipedia data loader](https://docs.llamaindex.ai/en/stable/examples/data_connectors/WikipediaDataLoaderDemo/) to create a RAG system out of the Wikipedia dataset.\n",
    "\n",
    "We will log the answers to the questions asked to the RAG system to Argilla and retrieve the logs to evaluate the performance of the RAG system.\n",
    "\n",
    "- Setting up the Argilla span handler for LlamaIndex.\n",
    "- Creating an index with a toy example on Wikipedia.\n",
    "- Create a RAG system out of the Argilla repository, ask questions, and automatically log the answers to Argilla.\n",
    "- Retrieve the logs from Argilla and evaluate the performance of the RAG system.\n",
    "\n",
    "This tutorial is based on the [Github Repository Reader](https://docs.llamaindex.ai/en/stable/examples/data_connectors/GithubRepositoryReaderDemo/) made by LlamaIndex.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "### Deploy the Argilla server¶\n",
    "\n",
    "If you already have deployed Argilla, you can skip this step. Otherwise, you can quickly deploy Argilla following [this guide](https://docs.argilla.io/latest/getting_started/quickstart/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the environment¶\n",
    "\n",
    "To complete this tutorial, you need to install this integration and a third-party library via pip.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qqq \"llama-index\" \\\n",
    "            \"llama-index-readers-wikipedia\" \\\n",
    "            \"argilla-llama-index>=2.1.0\" \\\n",
    "            \"wikipedia\" \\\n",
    "            \"llama-index-embeddings-huggingface\" \\\n",
    "            \"llama-index-llms-huggingface-api\" \\\n",
    "            \"huggingface_hub[inference]\" \\\n",
    "            \"argilla\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to set the OpenAI API key and the GitHub token. The OpenAI API key is required to run queries using GPT models, while the GitHub token ensures you have access to the repository you're using. Although the GitHub token might not be necessary for public repositories, it is still recommended.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the Argilla's LlamaIndex handler\n",
    "\n",
    "To easily log your data into Argilla within your LlamaIndex workflow, you only need to initialize the span handler and attach it to the Llama Index dispatcher. This ensured that the predictions obtained using Llama Index are automatically logged to the Argilla instance.\n",
    "\n",
    "- `dataset_name`: The name of the dataset. If the dataset does not exist, it will be created with the specified name. Otherwise, it will be updated.\n",
    "- `api_url`: The URL to connect to the Argilla instance.\n",
    "- `api_key`: The API key to authenticate with the Argilla instance.\n",
    "- `number_of_retrievals`: The number of retrieved documents to be logged. Defaults to 0.\n",
    "- `workspace_name`: The name of the workspace to log the data. By default, the first available workspace.\n",
    "\n",
    "> For more information about the credentials, check the documentation for [users](https://docs.argilla.io/latest/how_to_guides/user/) and [workspaces](https://docs.argilla.io/latest/how_to_guides/workspace/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ben/code/argilla-llama-index/.venv/lib/python3.11/site-packages/argilla/datasets/_resource.py:250: UserWarning: Workspace not provided. Using default workspace: argilla id: 31fb2d4c-46f8-4698-8dcb-27555e78c092\n",
      "  warnings.warn(f\"Workspace not provided. Using default workspace: {workspace.name} id: {workspace.id}\")\n"
     ]
    }
   ],
   "source": [
    "import argilla as rg\n",
    "import llama_index.core.instrumentation as instrument\n",
    "from argilla_llama_index import ArgillaHandler\n",
    "from llama_index.core import (\n",
    "    Settings,\n",
    "    VectorStoreIndex,\n",
    ")\n",
    "\n",
    "api_key = \"YOUR_API_KEY\"\n",
    "api_url = \"YOUR_API_URL\"\n",
    "dataset_name = \"wikipedia_query_llama_index\"\n",
    "\n",
    "span_handler = ArgillaHandler(\n",
    "    dataset_name=dataset_name,\n",
    "    api_url=api_url,\n",
    "    api_key=api_key,\n",
    "    number_of_retrievals=2,\n",
    ")\n",
    "\n",
    "dispatcher = instrument.get_dispatcher().add_span_handler(span_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.wikipedia import WikipediaReader\n",
    "\n",
    "# Initialize WikipediaReader\n",
    "reader = WikipediaReader()\n",
    "\n",
    "# Load data from Wikipedia\n",
    "documents = reader.load_data(pages=[\"Python (programming language)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the index and make some queries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a LlamaIndex index out of this document, and we can start querying the RAG system. We will use open source models from the huggingface hub. We will use two models:\n",
    "\n",
    "- Llama 3.1 8b instruct: This is an instruction tuned model by meta with 8 billion parameters. We will use Inference APIs to query this model.\n",
    "- bge-small-en-v1.5: This is a small model by EleutherAI for creating embeddings. We will run this model locally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "\n",
    "HF_TOKEN = \"YOUR_HUGGING_FACE_API_TOKEN\"\n",
    "\n",
    "# LLM settings\n",
    "Settings.llm = HuggingFaceInferenceAPI(\n",
    "    model_name=\"meta-llama/Llama-3.1-8B-Instruct\", token=HF_TOKEN\n",
    ")\n",
    "\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# Create the query engine with the same similarity top k as the number of retrievals\n",
    "query_engine = index.as_query_engine(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a LlamaIndex index, we can start querying the RAG system. We will use the `query` method to ask questions to the RAG system. \n",
    "\n",
    "Below we define a few queries that relate to the document we indexed. In a real-world scenario, you would ask questions that are relevant to the document you indexed and your use case. You might want to maintain these queries in a separate file or a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_about_python = [\n",
    "    \"Who invented python?\",\n",
    "    \"What is the latest version of python?\",\n",
    "    \"What is the license of python?\",\n",
    "    \"How is python different from other programming languages?\",\n",
    "    \"What is the python software foundation?\",\n",
    "    \"What is the python package index?\",\n",
    "    \"What is the python package manager?\",\n",
    "    \"When was python first released?\",\n",
    "]\n",
    "\n",
    "for question in queries_about_python:\n",
    "    response = query_engine.query(question)\n",
    "    print(response.response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated response will be automatically logged in our Argilla instance. Check it out! From Argilla you can quickly have a look at your predictions and annotate them, so you can combine both synthetic data and human feedback.\n",
    "\n",
    "![Argilla UI](images/llama_index_wikipedia.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect feedback from Argilla\n",
    "\n",
    "After visting the argilla UI and adding some feedback, we can retrieve the responses from Argilla and evaluate the performance of the RAG system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 3, 3, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "client = rg.Argilla(api_key=api_key, api_url=api_url)\n",
    "\n",
    "dataset = client.datasets(dataset_name)\n",
    "\n",
    "ratings = []\n",
    "responses = []\n",
    "feedbacks = []\n",
    "\n",
    "for record in dataset.records(with_responses=True):\n",
    "    response = record.fields[\"chat\"][-1].content\n",
    "    responses.append(response)\n",
    "    for response in record.responses:\n",
    "        if response.question_name == \"response-rating\":\n",
    "            ratings.append(response.value)\n",
    "        elif response.question_name == \"response-feedback\":\n",
    "            feedbacks.append(response.value)\n",
    "\n",
    "print(ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add feedback to prompts\n",
    "\n",
    "You could could use the feedback to improve the performance of the RAG system. For example, you could add the feedback to a prompt to create a few shot example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respond to this question based on the feedback provided about previous responses.\n",
      "\n",
      "Question: Who invented python?\n",
      "Rating: 2\n",
      "Response: 3.12.0\n",
      "Rating: 5\n",
      "Feedback: Good job!  You got it right!  The latest version of python is indeed 3.12.0.  Well done!  You must have read the text carefully.  Keep up the good work!  You are on a roll!  Keep it up!  You are doing great!  Keep going!  You are almost there!  Keep it up!  You are so close!  Keep going!  You did it!  You got it right!  Well done!  You must have read the text carefully.  Keep up the good work!  You are on a roll!  Keep it up!  You are doing great!  Keep going!  You are almost there!  Keep it up!  You are so close!  Keep going!  You did it!  You got it right!  Well done!  You must have read the text carefully.  Keep up the good work!  You are on a roll!  Keep it up!  You are doing great!  Keep going!  You are almost there!  Keep it up!  You are so close!  Keep going!  You did it!  You got\n",
      "Feedback: Should only be one sentence\n",
      "\n",
      "Question: What is the latest version of python?\n",
      "Rating: 3\n",
      "Response:  Guido van Rossum.  He began working on Python in the late 1980s as a successor to the ABC programming language.  He first released it in 1991 as Python 0.9.0.  He was the lead developer until 12 July 2018, when he announced his \"permanent vacation\" from his responsibilities as Python's \"benevolent dictator for life\" (BDFL).  He has since come out of retirement and is self-titled \"BDFL-emeritus\".  In January 2019, active Python core developers elected a five-member Steering Council to lead the project.  He is a Dutch computer programmer who is best known for designing and implementing the Python programming language.  He is also the creator of the popular Python programming language.  He is a highly respected figure in the programming community and is known for his contributions to the field of computer science.  He is a Dutch national and has lived in the United States since the 1990s.  He is a graduate of the University of Amsterdam and has a Ph.D. in computer science from the University of Amsterdam.  He is a fellow of the Association for Computing Machinery (ACM) and has received numerous awards for his contributions to\n",
      "Feedback: repetitive\n",
      "\n",
      "Question: What is the license of python?\n",
      "Rating: 3\n",
      "Response:  Python is open-source software released under the OSI-approved Python License.  It is also covered by the GNU General Public License (GPL) version 2.0.  The license is permissive, allowing users to freely use, modify, and distribute Python software without requiring them to disclose their modifications or pay royalties.  The license also requires that any modifications or derivative works be made available under the same license.  The license is widely used in the open-source community and is considered to be one of the most permissive open-source licenses available.  The license is compatible with the GNU General Public License (GPL) version 2.0 and other open-source licenses, such as the MIT License and the Apache License.  The license is a permissive license that allows users to freely use, modify, and distribute Python software without requiring them to disclose their modifications or pay royalties.  The license is widely used in the open-source community and is considered to be one of the most permissive open-source licenses available.  The license is compatible with the GNU General Public License (GPL) version 2.0 and other open-source licenses, such as the MIT License and the Apache License.  The license is a permissive license that allows users to freely use, modify, and\n",
      "Feedback: Way too long. just show the answer\n"
     ]
    }
   ],
   "source": [
    "FEW_SHOT_PROMPT = \"\"\"Respond to this question based on the feedback provided about previous responses.\"\"\"\n",
    "\n",
    "for question, rating, response, feedback in zip(queries_about_python, ratings, responses, feedbacks):\n",
    "\n",
    "    FEW_SHOT_PROMPT += f\"\\n\\nQuestion: {question}\\nRating: {rating}\\nResponse: {response}\\nFeedback: {feedback}\"\n",
    "    \n",
    "print(FEW_SHOT_PROMPT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
